{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*IMDB Dataset\n",
    "\n",
    "A benchmark dataset for sentiment analysis with labeled movie reviews (positive/negative).\n",
    "Lightweight, requiring fewer resources than larger datasets, ideal for initial NLP experiments.\n",
    "Simple and adaptable for tasks like review classification or sentiment detection.\n",
    "Widely accessible and used for fine-tuning pre-trained models like BERT/DistilBERT.\n",
    "\n",
    "*DistilBERT-base-uncased Model\n",
    "\n",
    "A smaller, faster version of BERT, retaining much of its performance for NLP tasks.\n",
    "Trained on diverse text, it captures context and semantics well.\n",
    "\"Uncased\" means case-insensitive, ideal for tasks where case doesn't matter.\n",
    "Efficient for IMDb review classification with competitive performance using fewer resources.\n",
    "\n",
    "*Evaluation (Accuracy)\n",
    "\n",
    "Accuracy is a clear, simple metric for sentiment analysis on the balanced IMDb dataset.\n",
    "Aligns with task objectives and is easy to communicate to non-technical stakeholders.\n",
    "While other metrics (precision, recall, F1) are useful, accuracy is often a baseline for classification.\n",
    "\n",
    "*PEFT (Low-Rank Adaptation)\n",
    "\n",
    "Low-rank adaptation reduces the computational cost of fine-tuning large models like DistilBERT.\n",
    "Focuses on relevant features, mitigates overfitting, and improves generalization.\n",
    "Enables faster fine-tuning and efficient sentiment analysis on IMDb with fewer resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "Load chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an  tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dd8d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (0.21.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2024.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d65af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from peft import (AutoPeftModelForSequenceClassification, LoraConfig, TaskType,\n",
    "                  get_peft_model)\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the required version of datasets in case you have an older version\n",
    "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
    "! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3584cb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88a70a148954ca28e99c0e19942effd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 24.2MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 34.3MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:01<00:00, 41.6MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db79177a22854bef8dd9abfc3e565022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381c5aec42e64e6089310bb59faddacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647dedb7c4bc4e67a7064f0e6e9846dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# show dataset details\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a795ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2, 'test': 2, 'unsupervised': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataset contained\n",
    "dataset.num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6205aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT Value(dtype='string', id=None)\n",
      "LABEL ClassLabel(names=['neg', 'pos'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# show training set text and labels\n",
    "print(\"TEXT\", dataset[\"train\"].features[f\"text\"])\n",
    "print(\"LABEL\", dataset[\"train\"].features[f\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "\n",
      "\n",
      "LABEL: 0\n"
     ]
    }
   ],
   "source": [
    "# show example test set text and labels\n",
    "print(\"TEXT:\", dataset[\"test\"][\"text\"][0])\n",
    "print(\"\\n\")\n",
    "print(\"LABEL:\", dataset[\"test\"][\"label\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d7a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up labels, label ids and number of labels\n",
    "labels = dataset[\"train\"].features[f\"label\"].names\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(labels)}\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "\n",
    "\n",
    "label_count = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fa39ba2d6a4aa7a4dbb2545523aea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54615bb916924be5a629c47fa14d46bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b24ca64a6540e4a089594dee9e065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54b5292a9474ef299f729e7c74efe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up tokenizer and paddiung\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e060f2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d435abd489814214b89ad381dbf50bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"neg\",\n",
      "    \"1\": \"pos\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"neg\": 0,\n",
      "    \"pos\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.36.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=label_count, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af491795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW INPUT: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "TOKENIZED OUTPUT: ['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '\"', 'controversial', '\"', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attention', '##s', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'sw', '##ede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'den', '##ize', '##ns', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'what', 'kills', 'me', 'about', 'i', 'am', 'curious', '-', 'yellow', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nu', '##dity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'\", 's', 'not', 'shot', 'like', 'some', 'cheap', '##ly', 'made', 'porn', '##o', '.', 'while', 'my', 'country', '##men', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nu', '##dity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', '.', 'even', 'ing', '##mar', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'do', 'com', '##men', '##d', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', '.', 'i', 'am', 'curious', '-', 'yellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', '(', 'no', 'pun', 'intended', ')', 'of', 'swedish', 'cinema', '.', 'but', 'really', ',', 'this', 'film', 'doesn', \"'\", 't', 'have', 'much', 'of', 'a', 'plot', '.', '[SEP]']\n",
      "TOKEN_IDS: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, None]\n"
     ]
    }
   ],
   "source": [
    "# tokenize example text\n",
    "tokenized_input = tokenizer(dataset[\"train\"][0][\"text\"], truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "print(\"RAW INPUT:\", dataset[\"train\"][0][\"text\"])\n",
    "print(\"TOKENIZED OUTPUT:\", tokens)\n",
    "print(\"TOKEN_IDS:\", tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c59624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([CLS],None), (i,0), (rented,1), (i,2), (am,3), (curious,4), (-,5), (yellow,6), (from,7), (my,8), (video,9), (store,10), (because,11), (of,12), (all,13), (the,14), (controversy,15), (that,16), (surrounded,17), (it,18), (when,19), (it,20), (was,21), (first,22), (released,23), (in,24), (1967,25), (.,26), (i,27), (also,28), (heard,29), (that,30), (at,31), (first,32), (it,33), (was,34), (seized,35), (by,36), (u,37), (.,38), (s,39), (.,40), (customs,41), (if,42), (it,43), (ever,44), (tried,45), (to,46), (enter,47), (this,48), (country,49), (,,50), (therefore,51), (being,52), (a,53), (fan,54), (of,55), (films,56), (considered,57), (\",58), (controversial,59), (\",60), (i,61), (really,62), (had,63), (to,64), (see,65), (this,66), (for,67), (myself,68), (.,69), (<,70), (br,71), (/,72), (>,73), (<,74), (br,75), (/,76), (>,77), (the,78), (plot,79), (is,80), (centered,81), (around,82), (a,83), (young,84), (swedish,85), (drama,86), (student,87), (named,88), (lena,89), (who,90), (wants,91), (to,92), (learn,93), (everything,94), (she,95), (can,96), (about,97), (life,98), (.,99), (in,100), (particular,101), (she,102), (wants,103), (to,104), (focus,105), (her,106), (attention,107), (##s,107), (to,108), (making,109), (some,110), (sort,111), (of,112), (documentary,113), (on,114), (what,115), (the,116), (average,117), (sw,118), (##ede,118), (thought,119), (about,120), (certain,121), (political,122), (issues,123), (such,124), (as,125), (the,126), (vietnam,127), (war,128), (and,129), (race,130), (issues,131), (in,132), (the,133), (united,134), (states,135), (.,136), (in,137), (between,138), (asking,139), (politicians,140), (and,141), (ordinary,142), (den,143), (##ize,143), (##ns,143), (of,144), (stockholm,145), (about,146), (their,147), (opinions,148), (on,149), (politics,150), (,,151), (she,152), (has,153), (sex,154), (with,155), (her,156), (drama,157), (teacher,158), (,,159), (classmates,160), (,,161), (and,162), (married,163), (men,164), (.,165), (<,166), (br,167), (/,168), (>,169), (<,170), (br,171), (/,172), (>,173), (what,174), (kills,175), (me,176), (about,177), (i,178), (am,179), (curious,180), (-,181), (yellow,182), (is,183), (that,184), (40,185), (years,186), (ago,187), (,,188), (this,189), (was,190), (considered,191), (pornographic,192), (.,193), (really,194), (,,195), (the,196), (sex,197), (and,198), (nu,199), (##dity,199), (scenes,200), (are,201), (few,202), (and,203), (far,204), (between,205), (,,206), (even,207), (then,208), (it,209), (',210), (s,211), (not,212), (shot,213), (like,214), (some,215), (cheap,216), (##ly,216), (made,217), (porn,218), (##o,218), (.,219), (while,220), (my,221), (country,222), (##men,222), (mind,223), (find,224), (it,225), (shocking,226), (,,227), (in,228), (reality,229), (sex,230), (and,231), (nu,232), (##dity,232), (are,233), (a,234), (major,235), (staple,236), (in,237), (swedish,238), (cinema,239), (.,240), (even,241), (ing,242), (##mar,242), (bergman,243), (,,244), (arguably,245), (their,246), (answer,247), (to,248), (good,249), (old,250), (boy,251), (john,252), (ford,253), (,,254), (had,255), (sex,256), (scenes,257), (in,258), (his,259), (films,260), (.,261), (<,262), (br,263), (/,264), (>,265), (<,266), (br,267), (/,268), (>,269), (i,270), (do,271), (com,272), (##men,272), (##d,272), (the,273), (filmmakers,274), (for,275), (the,276), (fact,277), (that,278), (any,279), (sex,280), (shown,281), (in,282), (the,283), (film,284), (is,285), (shown,286), (for,287), (artistic,288), (purposes,289), (rather,290), (than,291), (just,292), (to,293), (shock,294), (people,295), (and,296), (make,297), (money,298), (to,299), (be,300), (shown,301), (in,302), (pornographic,303), (theaters,304), (in,305), (america,306), (.,307), (i,308), (am,309), (curious,310), (-,311), (yellow,312), (is,313), (a,314), (good,315), (film,316), (for,317), (anyone,318), (wanting,319), (to,320), (study,321), (the,322), (meat,323), (and,324), (potatoes,325), ((,326), (no,327), (pun,328), (intended,329), (),330), (of,331), (swedish,332), (cinema,333), (.,334), (but,335), (really,336), (,,337), (this,338), (film,339), (doesn,340), (',341), (t,342), (have,343), (much,344), (of,345), (a,346), (plot,347), (.,348), ([SEP],None), "
     ]
    }
   ],
   "source": [
    "# show tokenized example text\n",
    "for token, word_id in zip(tokens, tokenized_input.word_ids()):\n",
    "\n",
    "    print(f\"({token},{word_id})\", end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53df74ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8bf328a80f40fc95d44f345548d068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15a49fcb2e4f659917907ad0ac593b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0503499b25084640ad5ec164c3b7f1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up tokenizer for corpus\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_input = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f910416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "0\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# check tokenizer\n",
    "print(tokenized_input[\"train\"][0][\"text\"])\n",
    "print(tokenized_input[\"train\"][0][\"label\"])\n",
    "print(tokenized_input[\"train\"][0][\"input_ids\"])\n",
    "print(tokenized_input[\"train\"][0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73421bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the pad token ID from the tokenizer to the pad_token_id attribute of the model's configuration.\n",
    "# This ensures consistency between the tokenizer and the model during tokenization and padding operations.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06ba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code iterates over all parameters of the base model within a larger neural network model (presumably a pre-trained model).\n",
    "# It sets the requires_grad attribute of each parameter to False, effectively freezing them from being updated during the training process.\n",
    "# This is necessary when fine-tuning a pre-trained model where we want to keep the parameters of the base model fixed while only updating the parameters of the added layers or the head of the model.\n",
    "# By setting requires_grad to False, we prevent gradients from being computed and accumulated for these parameters during backpropagation, thus ensuring that they remain unchanged.\n",
    "for param in model.base_model.parameters():\n",
    "\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee510c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9364c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of class labels\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b754d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcd66f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c126247f4a40b888e80e81cdddeb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up accuracy as metric function\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3fdecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training function \n",
    "def train_model(model, output_dir, train_dataset, eval_dataset, tokenizer, compute_metrics,train_req):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    if train_req == True:\n",
    "        trainer.train()\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4122b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train foundation model on training data\n",
    "trainer_foundation = train_model(\n",
    "    model, './foundation_model', tokenized_input[\"train\"], tokenized_input[\"test\"], tokenizer, compute_accuracy,train_req=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5aa45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 06:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6949097514152527,\n",
       " 'eval_accuracy': 0.478,\n",
       " 'eval_runtime': 408.0618,\n",
       " 'eval_samples_per_second': 61.265,\n",
       " 'eval_steps_per_second': 1.916}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate foundation model on test data\n",
    "trainer_foundation.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "Creating a PEFT model from your loaded model, runninng a training loop, and savingthe PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,405,444 || all params: 67,768,324 || trainable%: 2.073895172617815\n"
     ]
    }
   ],
   "source": [
    "# set up LoRA (low-Rank Adaption)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=[\n",
    "                         'q_lin', 'k_lin', 'v_lin'], inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 22:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.283427</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./lora_model/checkpoint-782 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "# train LoRA-finetuned model of training data\n",
    "trainer_finetuning = train_model(\n",
    "    lora_model, './lora_model', tokenized_input[\"train\"], tokenized_input[\"test\"], tokenizer, compute_accuracy,train_req= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 07:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28342723846435547,\n",
       " 'eval_accuracy': 0.8798,\n",
       " 'eval_runtime': 433.812,\n",
       " 'eval_samples_per_second': 57.629,\n",
       " 'eval_steps_per_second': 1.803,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate LoRA-finetuned model of test data\n",
    "trainer_finetuning.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "lora_model.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "best_model = AutoPeftModelForSequenceClassification.from_pretrained(\"lora_model\",  num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): MultiHeadSelfAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (k_lin): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_lin): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract aim data\n",
    "infer_data = dataset[\"unsupervised\"][\"text\"][:5]\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)', 'When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.', 'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.', \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\", \"After seeing Point of No Return (a great movie) and being told that the original was better, I was certainly thrilled to see that one of the indie film channels was running La Femme Nikita. Then I saw the movie. Ouch! This was a major let-down.<br /><br />Nikita herself reminds me of Jar Jar Binks more than any other character I've seen recently. She comes across entirely as comic relief. The movie simply has nothing to recommend it besides the core concept of an evil, inhuman character paradoxically learning to be human while training as an assassin, and that concept failed miserably in Nikita due to the poor writing of the title role.\"]\n"
     ]
    }
   ],
   "source": [
    "print(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_infer_data = tokenizer(infer_data, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 2074, 1037, 9062, 2210, 6323, 1012, 1996, 2377, 1010, 1996, 5896, 2024, 6581, 1012, 1045, 2064, 2102, 12826, 2023, 3185, 2007, 2505, 2842, 1010, 2672, 3272, 1996, 3185, 1000, 6506, 1000, 6919, 2135, 2209, 2011, 3744, 17738, 1998, 10829, 3417, 2386, 1012, 2021, 1012, 1012, 1012, 2054, 2064, 1045, 2360, 2055, 2023, 2028, 1029, 2023, 2003, 1996, 2190, 3185, 4776, 11968, 9386, 6784, 2038, 2412, 2209, 1999, 1006, 2156, 3531, 1000, 12784, 2732, 7138, 1000, 1010, 2016, 1005, 1055, 4092, 2394, 2045, 1007, 2000, 2156, 2054, 1045, 2812, 1012, 1996, 2466, 1997, 2402, 7196, 2611, 29106, 1010, 2579, 2046, 1996, 2139, 18098, 10696, 2094, 2088, 1997, 1996, 3595, 2231, 2749, 2038, 2042, 17077, 2058, 2109, 2011, 4841, 1012, 2196, 2568, 1996, 1000, 2391, 1997, 2053, 2709, 1000, 1998, 2926, 1996, 1000, 2474, 26893, 29106, 1000, 2694, 2186, 1012, 2027, 3685, 12826, 1996, 2434, 2903, 2033, 999, 11669, 2122, 6876, 1012, 4965, 2023, 2028, 1010, 2079, 2025, 9278, 2009, 1010, 4965, 2009, 1012, 18411, 2860, 2022, 8059, 1997, 1996, 4942, 27430, 1997, 1996, 2474, 2194, 2029, 1000, 17637, 1000, 1996, 2149, 2713, 1012, 2054, 1037, 29591, 999, 2065, 2017, 2064, 2102, 3305, 2413, 1010, 2131, 1037, 9188, 2544, 1012, 2021, 2017, 1005, 2222, 9038, 2101, 1024, 1007, 102], [101, 2043, 1045, 2360, 2023, 2003, 2026, 8837, 2143, 1997, 2035, 2051, 1010, 2008, 7615, 2003, 2025, 2000, 2022, 2579, 8217, 1012, 1045, 2763, 3422, 2521, 2205, 2116, 3152, 2084, 2003, 7965, 2005, 2033, 1010, 1998, 2031, 3866, 3243, 1037, 2261, 1997, 2068, 1012, 1045, 2034, 2387, 1000, 2474, 26893, 29106, 1000, 3053, 2702, 2086, 3283, 1010, 1998, 2009, 2145, 9020, 2000, 2022, 2026, 7619, 8837, 1012, 2339, 1029, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2023, 2003, 2062, 2084, 2019, 11757, 2358, 8516, 4509, 1998, 7916, 10874, 1012, 12776, 2022, 7092, 1005, 1055, 2307, 22012, 2005, 17727, 8586, 21170, 3257, 1010, 4827, 1010, 1998, 6413, 8192, 1997, 2189, 3084, 2023, 1037, 2200, 3422, 3085, 2143, 1012, 2021, 2009, 2003, 4776, 11968, 9386, 6784, 1005, 1055, 3819, 14259, 1997, 1037, 3375, 2839, 2040, 21743, 2013, 1037, 2540, 3238, 6359, 2046, 1037, 29353, 1010, 17026, 2402, 2450, 2008, 3084, 2023, 2143, 3376, 1012, 1045, 2064, 1005, 1056, 2562, 2026, 2159, 2125, 1997, 2014, 2043, 2016, 2003, 2006, 3898, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2031, 2464, 2195, 1997, 12776, 2022, 7092, 1005, 1055, 3152, 2164, 1000, 10798, 1000, 1010, 1000, 1996, 2658, 1000, 1010, 1998, 1996, 29348, 1000, 3587, 5783, 1000, 1010, 1998, 1000, 29106, 1000, 2003, 2302, 1037, 4797, 1010, 2521, 6020, 2000, 2151, 1997, 2122, 1012, 2348, 2023, 2143, 2038, 13800, 3787, 1010, 2009, 2003, 4821, 5186, 17772, 1012, 2009, 2003, 1996, 2466, 1997, 1037, 2711, 2040, 2003, 10311, 1998, 21442, 6895, 3238, 1010, 2040, 4821, 3310, 2000, 5382, 2014, 2219, 8438, 1998, 2014, 2219, 3167, 2373, 1012, 2008, 1010, 2000, 2033, 2003, 5186, 18988, 1012, 2065, 2045, 2003, 3246, 2005, 29106, 1010, 2045, 2003, 3246, 2005, 2035, 1997, 2149, 1012, 102], [101, 1045, 2387, 2023, 3185, 2138, 1045, 2572, 1037, 4121, 5470, 1997, 1996, 2694, 2186, 1997, 1996, 2168, 2171, 4626, 6060, 4241, 14289, 2483, 1998, 9004, 4267, 1012, 1996, 3185, 2001, 2428, 2204, 1998, 1045, 2387, 2129, 1996, 2694, 2265, 2003, 2241, 2006, 1996, 3185, 1012, 1037, 2261, 4178, 1997, 1996, 2694, 2186, 2234, 3495, 2013, 1996, 3185, 1998, 2037, 14402, 2001, 6429, 1012, 2000, 2562, 2477, 2460, 1010, 2151, 5470, 1997, 1996, 3185, 2038, 2000, 3422, 1996, 2186, 1998, 2151, 5470, 1997, 1996, 2186, 2442, 2156, 1996, 2434, 29106, 1012, 102], [101, 2108, 2008, 1996, 2069, 3097, 3152, 1045, 2788, 2066, 2732, 1037, 2887, 2711, 1999, 1037, 8903, 4848, 2040, 10188, 2229, 2210, 4714, 3121, 1998, 7286, 1010, 1045, 2018, 2152, 8069, 2005, 2023, 3185, 1012, 1045, 2245, 2008, 2023, 2001, 1037, 3185, 2008, 2876, 1005, 1056, 2404, 2033, 2000, 3637, 1012, 3308, 999, 4627, 2125, 2007, 1037, 9748, 1010, 3100, 1010, 2085, 2016, 1005, 1055, 1999, 2731, 1010, 10303, 1010, 2016, 1005, 1055, 2019, 12025, 1010, 1045, 1005, 1049, 2145, 2007, 2017, 1010, 2821, 1010, 2085, 2016, 1005, 1055, 2383, 2023, 7191, 21883, 1998, 2016, 2064, 1005, 1056, 5630, 2065, 2016, 7459, 2014, 6898, 2030, 2014, 11486, 1010, 1062, 13213, 13213, 1012, 1012, 1012, 1012, 2821, 2092, 1010, 2067, 2000, 27911, 2050, 999, 102], [101, 2044, 3773, 2391, 1997, 2053, 2709, 1006, 1037, 2307, 3185, 1007, 1998, 2108, 2409, 2008, 1996, 2434, 2001, 2488, 1010, 1045, 2001, 5121, 16082, 2000, 2156, 2008, 2028, 1997, 1996, 10271, 2143, 6833, 2001, 2770, 2474, 26893, 29106, 1012, 2059, 1045, 2387, 1996, 3185, 1012, 15068, 2818, 999, 2023, 2001, 1037, 2350, 2292, 1011, 2091, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 29106, 2841, 15537, 2033, 1997, 15723, 15723, 8026, 5705, 2062, 2084, 2151, 2060, 2839, 1045, 1005, 2310, 2464, 3728, 1012, 2016, 3310, 2408, 4498, 2004, 5021, 4335, 1012, 1996, 3185, 3432, 2038, 2498, 2000, 16755, 2009, 4661, 1996, 4563, 4145, 1997, 2019, 4763, 1010, 29582, 2839, 20506, 15004, 4083, 2000, 2022, 2529, 2096, 2731, 2004, 2019, 12025, 1010, 1998, 2008, 4145, 3478, 28616, 6906, 6321, 1999, 29106, 2349, 2000, 1996, 3532, 3015, 1997, 1996, 2516, 2535, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0e3d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "\n",
    "for text in range(len(tokenized_infer_data['input_ids'])):\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(\n",
    "            tokenized_infer_data['input_ids'][text]).unsqueeze(0).to(device)\n",
    "        outputs = best_model(input_ids=input_ids)\n",
    "        logits = outputs.logits.to(device)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_classes.append(predicted_class)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\"input_ids\": infer_data, \"predicted_class\": predicted_classes})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da6d3a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is just a precious little diamond. The pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I say this is my favourite film of all ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I saw this movie because I am a huge fan of th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Being that the only foreign films I usually li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After seeing Point of No Return (a great movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  predicted_class\n",
       "0  This is just a precious little diamond. The pl...                0\n",
       "1  When I say this is my favourite film of all ti...                1\n",
       "2  I saw this movie because I am a huge fan of th...                1\n",
       "3  Being that the only foreign films I usually li...                0\n",
       "4  After seeing Point of No Return (a great movie...                0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display DataFrame\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
